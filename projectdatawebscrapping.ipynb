{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6af2061-f752-4716-93ab-f4aedc1300c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31012b1f-9a24-41aa-9508-d7e6bcfa1341",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This code sets up the necessary tools and headers to scrape web data. It imports libraries for sending HTTP requests, parsing HTML, and handling data. Additionally, it defines a user agent header to mimic a web browser, which can be useful to avoid getting blocked by some websites.\n",
    "\n",
    "## import requests\n",
    "\n",
    "This line imports the requests module, which is a popular Python module used to send HTTP requests to websites.\n",
    "\n",
    "## from bs4 import BeautifulSoup \n",
    "This line imports BeautifulSoup from the bs4 module. BeautifulSoup is a library that is used for web scraping purposes to pull the data out of HTML and XML files. It creates a parse tree that can be used to extract data in a hierarchical and more readable manner.\n",
    "\n",
    "## import os\n",
    "\n",
    "This line imports the os module, which provides a way of interacting with the operating system. This could be used for tasks like creating directories, reading environment variables, etc.\n",
    "\n",
    "## headers = {...}\n",
    "\n",
    "This line defines a dictionary called headers with a 'User-Agent' key. The value of this key is a string that represents a user agent string.\n",
    "\n",
    "The user agent string is used to tell the server about the browser and operating system of the user. Some websites serve different content based on the user agent or even block certain user agents (often to prevent scraping). By defining a common browser's user agent string, this code is trying to mimic a real browser request to potentially avoid blocks or get the same content a real user would see.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d847e3-889b-44c7-a917-491fc88281e5",
   "metadata": {},
   "source": [
    "Extracting Flats/Apartments\n",
    "- Your_Project_Directory\n",
    "  - Data\n",
    "    - City\n",
    "      - Flats\n",
    "      - Societies\n",
    "      - Residential\n",
    "      - Independent House"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5b5a18-de1d-4e8c-9879-c871399b19ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to change as per your requirement - city name\n",
    "# Match with 99acers site like for chandighars flats data site is : https://www.99acres.com/flats-in-chandigarh-ffid\n",
    "# Taking value of city as 'chandigarh'\n",
    "City = 'chandigarh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8424967b-c9b2-4519-8ee5-46ca53346321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Agent\n",
    "# Headers set like below:\n",
    "# User Agent\n",
    "headers = {\n",
    "    'authority': 'www.99acres.com',\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "    'accept-language': 'en-US,en;q=0.9',\n",
    "    'cache-control': 'no-cache',\n",
    "    'dnt': '1',\n",
    "    'pragma': 'no-cache',\n",
    "    'referer': f'https://www.99acres.com/flats-in-{City}-ffid-page',\n",
    "    'sec-ch-ua': '\"Chromium\";v=\"107\", \"Not;A=Brand\";v=\"8\"',\n",
    "    'sec-ch-ua-mobile': '?0',\n",
    "    'sec-ch-ua-platform': '\"macOS\"',\n",
    "    'sec-fetch-dest': 'document',\n",
    "    'sec-fetch-mode': 'navigate',\n",
    "    'sec-fetch-site': 'same-origin',\n",
    "    'sec-fetch-user': '?1',\n",
    "    'upgrade-insecure-requests': '1',\n",
    "    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/527.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebca59a6-944d-4143-ae9a-6c6482983dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If folder structures are in already created no need to run it.\n",
    "\n",
    "import os\n",
    "\n",
    "# Define the path to your project directory\n",
    "project_dir = '/content/drive/MyDrive/DSMP/Case Studies/Real estate/'\n",
    "\n",
    "# Define the subdirectories\n",
    "subdirectories = ['Data', f'Data/{City}', f'Data/{City}/Flats', f'Data/{City}/Societies', f'Data/{City}/Residential', f'Data/{City}/Independent House']\n",
    "\n",
    "# Create the directory structure\n",
    "for subdir in subdirectories:\n",
    "    dir_path = os.path.join(project_dir, subdir)\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "        print(f\"Created directory: {dir_path}\")\n",
    "    else:\n",
    "        print(f\"Directory already exists: {dir_path}\")\n",
    "\n",
    "# Now, your directory structure is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f47a13-0dbf-479f-a59a-93e970cec6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "from requests.exceptions import ReadTimeout, RequestException\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "City = \"chandigarh\"\n",
    "\n",
    "ua = UserAgent()\n",
    "\n",
    "def get_headers():\n",
    "    return {\n",
    "        \"User-Agent\": ua.random,\n",
    "        \"Accept-Language\": \"en-IN,en;q=0.9\",\n",
    "        \"Referer\": \"https://www.google.com/\"\n",
    "    }\n",
    "\n",
    "# ---------------- INPUT ----------------\n",
    "start = int(input(\"Enter page number to start from: \"))\n",
    "end = start + 10\n",
    "\n",
    "# ---------------- SESSION ----------------\n",
    "session = requests.Session()\n",
    "session.headers.update(get_headers())\n",
    "\n",
    "req = 0\n",
    "pageNumber = start\n",
    "flats = pd.DataFrame()\n",
    "\n",
    "# ---------------- SCRAPER ----------------\n",
    "try:\n",
    "    while pageNumber < end:\n",
    "        print(f\"\\nScraping page {pageNumber}\")\n",
    "        url = f\"https://www.99acres.com/flats-in-{City}-ffid-page-{pageNumber}\"\n",
    "\n",
    "        try:\n",
    "            page = session.get(url, timeout=(5, 15))\n",
    "        except ReadTimeout:\n",
    "            print(\"Listing page timeout, sleeping...\")\n",
    "            time.sleep(30)\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        sections = soup.select_one('div[data-label=\"SEARCH\"]')\n",
    "\n",
    "        if not sections:\n",
    "            print(\"Blocked / empty page detected\")\n",
    "            break\n",
    "\n",
    "        properties = sections.select('section[data-hydration-on-demand=\"true\"]')\n",
    "        i = 0\n",
    "\n",
    "        for prop in properties:\n",
    "            try:\n",
    "                property_name = prop.select_one('a.srpTuple__propertyName').text.strip()\n",
    "                link = prop.select_one('a.srpTuple__propertyName')['href']\n",
    "                society = prop.select_one('#srp_tuple_society_heading').text.strip()\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            # ---- DETAIL PAGE ----\n",
    "            try:\n",
    "                dpage = session.get(link, timeout=(5, 15))\n",
    "            except ReadTimeout:\n",
    "                time.sleep(15)\n",
    "                continue\n",
    "\n",
    "            dSoup = BeautifulSoup(dpage.content, \"html.parser\")\n",
    "            req += 1\n",
    "\n",
    "            def safe_text(selector):\n",
    "                try:\n",
    "                    return dSoup.select_one(selector).text.strip()\n",
    "                except:\n",
    "                    return \"\"\n",
    "\n",
    "            property_data = {\n",
    "                \"property_name\": property_name,\n",
    "                \"link\": link,\n",
    "                \"society\": society,\n",
    "                \"price\": safe_text(\"#pdPrice2\"),\n",
    "                \"area\": safe_text(\"#srp_tuple_price_per_unit_area\"),\n",
    "                \"areaWithType\": safe_text(\"#factArea\"),\n",
    "                \"bedRoom\": safe_text(\"#bedRoomNum\"),\n",
    "                \"bathroom\": safe_text(\"#bathroomNum\"),\n",
    "                \"balcony\": safe_text(\"#balconyNum\"),\n",
    "                \"additionalRoom\": safe_text(\"#additionalRooms\"),\n",
    "                \"address\": safe_text(\"#address\"),\n",
    "                \"floorNum\": safe_text(\"#floorNumLabel\"),\n",
    "                \"facing\": safe_text(\"#facingLabel\"),\n",
    "                \"agePossession\": safe_text(\"#agePossessionLbl\"),\n",
    "                \"property_id\": safe_text(\"#Prop_Id\")\n",
    "            }\n",
    "\n",
    "            # Nearby locations\n",
    "            try:\n",
    "                property_data[\"nearbyLocations\"] = [\n",
    "                    i.text.strip() for i in dSoup.select(\n",
    "                        \"div.NearByLocation__tagWrap span.NearByLocation__infoText\"\n",
    "                    )\n",
    "                ]\n",
    "            except:\n",
    "                property_data[\"nearbyLocations\"] = \"\"\n",
    "\n",
    "            # Description\n",
    "            property_data[\"description\"] = safe_text(\"#description\")\n",
    "\n",
    "            # Furnish details\n",
    "            try:\n",
    "                property_data[\"furnishDetails\"] = [\n",
    "                    i.text.strip() for i in dSoup.select(\"#FurnishDetails li\")\n",
    "                ]\n",
    "            except:\n",
    "                property_data[\"furnishDetails\"] = \"\"\n",
    "\n",
    "            # Features\n",
    "            try:\n",
    "                property_data[\"features\"] = [\n",
    "                    i.text.strip() for i in dSoup.select(\"#features li\")\n",
    "                ]\n",
    "            except:\n",
    "                property_data[\"features\"] = \"\"\n",
    "\n",
    "            flats = pd.concat(\n",
    "                [flats, pd.DataFrame([property_data])],\n",
    "                ignore_index=True\n",
    "            )\n",
    "\n",
    "            i += 1\n",
    "            time.sleep(random.uniform(2, 6))\n",
    "\n",
    "        print(f\"Page {pageNumber} â†’ {i} properties scraped\")\n",
    "        pageNumber += 1\n",
    "        time.sleep(random.uniform(15, 30))\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"ERROR OCCURRED:\", e)\n",
    "\n",
    "finally:\n",
    "    # ---------------- SAVE DATA ----------------\n",
    "    save_path = f\"flats_{City}_page_{start}_to_{pageNumber-1}.csv\"\n",
    "    flats.to_csv(save_path, index=False)\n",
    "    print(f\"\\nData saved to: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a76f077-94e5-4f1b-8a22-23031e1aa52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fake-useragent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fbf4a6-f419-4c1e-95a7-76172adc21a6",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "The code scrapes property data from the website \"99acres.com\" for apartments in Gurgaon. It navigates through a range of pages, extracts details of each property, and saves the data to a CSV file. The script is designed to handle potential errors gracefully, using try and except blocks to manage missing data, and introduces pauses to avoid making rapid requests and potentially getting blocked by the website.\n",
    "\n",
    "## Initialization of Variables:\n",
    "\n",
    "start and end specify the range of web pages to scrape.\n",
    "csv_file defines the path to the CSV file where data will be saved.\n",
    "pageNumber starts from the initial value of start and will be incremented to navigate through the pages.\n",
    "req counts the number of HTTP requests made.\n",
    "## Loop for Page Navigation:\n",
    "\n",
    "The while loop is used to navigate through each page in the range from start to end.\n",
    "Inside this loop, the URL of the page to be scraped is constructed using the pageNumber.\n",
    "An HTTP GET request is made to retrieve the content of the page, and the content is then parsed using BeautifulSoup.\n",
    "## Loop for Property Extraction:\n",
    "\n",
    "The nested for loop navigates through individual property sections on the current page.\n",
    "The script attempts to extract the property name, its link, and its society name.\n",
    "If any of these attributes are missing, it skips to the next property.\n",
    "## Detail Extraction:\n",
    "\n",
    "For each property, an HTTP request is made to its detail page.\n",
    "The code then attempts to extract various property details like price, area, bedroom count, bathroom count, balcony count, address, and many other attributes. If any attribute is missing, the code handles it gracefully, assigning an empty string or an empty list as appropriate.\n",
    "## Creating and Saving Data:\n",
    "\n",
    "All extracted details are stored in a dictionary named property_data.\n",
    "This dictionary is then converted to a temporary DataFrame temp_df.\n",
    "The data is appended to a main DataFrame flats and also saved to the CSV file. If the file already exists, the new data is appended without writing the headers again.\n",
    "## Request Management:\n",
    "\n",
    "To avoid making too many rapid requests (which can lead to IP bans), the script introduces pauses.\n",
    "Every 4 requests, it pauses for 10 seconds. Every 15 requests, it pauses for 50 seconds.\n",
    "Page Counter and Loop Increment:\n",
    "\n",
    "After scraping all properties on a page, the code prints the page number and the number of properties processed.\n",
    "pageNumber is incremented to move to the next page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39880ca-3c16-4b68-99f0-4f46aee4bba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to combine multiple csv file is one file.\n",
    "\n",
    "def combine_csv_files(folder_path, combined_file_path):\n",
    "    combined_data = pd.DataFrame()  # Create an empty DataFrame to hold the combined data\n",
    "\n",
    "    # Iterate through all CSV files in the folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            print('file_path')\n",
    "            # Read the data from the current CSV file\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Append the data to the combined DataFrame\n",
    "            combined_data = combined_data.append(df, ignore_index=True)\n",
    "\n",
    "            # Delete the original CSV file\n",
    "            os.remove(file_path)\n",
    "\n",
    "    # Save the combined data to a new CSV file\n",
    "    combined_data.to_csv(combined_file_path, index=False)\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "# Replace with the actual folder path\n",
    "folder_path = '/content/drive/MyDrive/DSMP/Case Studies/Real estate/flats_appartment'\n",
    "\n",
    "# Replace with the desired combined file path\n",
    "combined_file_path = '/content/drive/MyDrive/DSMP/Case Studies/Real estate/flats_appartment/flats.csv'\n",
    "\n",
    "combine_csv_files(folder_path, combined_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19b2c6b-b0f1-4be4-9c6a-fc94b4cfc5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview: \n",
    "The function combine_csv_files combines all the CSV files located in a specified folder into a single CSV file. After appending the data from each individual file to the combined file, the original file is deleted.\n",
    "\n",
    "## Function Definition:\n",
    "\n",
    "### combine_csv_files(folder_path, combined_file_path):\n",
    "\n",
    "### folder_path: Path to the folder containing the CSV files you want to combine.\n",
    "combined_file_path: Path where the combined CSV file should be saved.\n",
    "### Initialize an Empty DataFrame:\n",
    "\n",
    "combined_data = pd.DataFrame(): An empty DataFrame combined_data is created to hold all the data from the individual CSV files.\n",
    "Iterate Through CSV Files:\n",
    "\n",
    "The for loop iterates over each file in the directory specified by folder_path.\n",
    "Within the loop, the code checks if the current file ends with .csv to ensure that only CSV files are processed.\n",
    "Read and Append Data:\n",
    "\n",
    "file_path = os.path.join(folder_path, file_name): Constructs the full path to the current CSV file.\n",
    "df = pd.read_csv(file_path): Reads the data from the current CSV file into a DataFrame df.\n",
    "combined_data = combined_data.append(df, ignore_index=True): Appends the data from df to the combined_data DataFrame. The ignore_index=True parameter ensures that the index is reset and continuous in the combined data.\n",
    "Delete the Original CSV File:\n",
    "\n",
    "os.remove(file_path): Deletes the original CSV file after its data has been appended to the combined data. This step helps in conserving storage space.\n",
    "Save the Combined Data:\n",
    "\n",
    "combined_data.to_csv(combined_file_path, index=False): Writes the combined_data DataFrame to a new CSV file at the specified combined_file_path. The parameter index=False ensures that the DataFrame's index is not written to the CSV.\n",
    "### Example Usage:\n",
    "\n",
    "The provided paths (folder_path and combined_file_path) specify the location of the individual CSV files and the path for the combined CSV file, respectively.\n",
    "Calling the combine_csv_files function with these paths will combine all CSV files in the specified folder and save the combined data to the desired location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314625a0-ec42-44f3-8947-7f723f326d34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b36727c-1123-48da-8888-d2055cd4be2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca066dd1-eac1-45dc-8185-b3bf2545e8be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
